{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e41c8a42",
   "metadata": {},
   "source": [
    "# This notebook train the Neural Network model using all training data by trunk, and evalute the Average Precision (AP) and other matrics by trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68984b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d664c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_files(input_list):\n",
    "    out_list=[]\n",
    "    if type(input_list)==type(None):\n",
    "        return out_list\n",
    "    for item in input_list:\n",
    "        \n",
    "        if os.path.isdir(item):\n",
    "            out_list.extend(list(Path(item).rglob(\"*.npz\")))\n",
    "\n",
    "        elif item[-4:]=='.npz':\n",
    "            out_list.append(item)        \n",
    "    \n",
    "    random.seed(0)\n",
    "    random.shuffle(out_list)\n",
    "    return out_list\n",
    "\n",
    "def read_from_file(file_name):\n",
    "    data=np.load(file_name)\n",
    "    return data['morgan'], data['protein'], data['labels']\n",
    "\n",
    "def generate_batches(files, batch_size=1024):\n",
    "    counter = 0\n",
    "    \n",
    "    print_freq=max(1, len(files)//10)\n",
    "    \n",
    "    while counter<len(files):\n",
    "        file_name = files[counter]\n",
    "\n",
    "        counter +=1\n",
    "        \n",
    "        data=read_from_file(file_name)\n",
    "\n",
    "        morgan, protein, labels=data\n",
    "        batch_size=max(batch_size,1)\n",
    "        for local_index in range(0, labels.shape[0], batch_size):\n",
    "            batch_morgan=morgan[local_index:(local_index + batch_size)]\n",
    "            batch_protein=protein[local_index:(local_index + batch_size)]\n",
    "            batch_labels=labels[local_index:(local_index + batch_size)]          \n",
    "\n",
    "            yield batch_morgan, batch_protein, batch_labels\n",
    "        \n",
    "        if counter%print_freq==0:\n",
    "            print('.', end='',flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cccb93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2f847a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim, Tensor\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1=nn.Linear(1027,2048)\n",
    "        #self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2=nn.Linear(2048,1024)\n",
    "        self.fc3=nn.Linear(1024,512)\n",
    "        self.fc4=nn.Linear(512,128)\n",
    "        #self.dropout3 = nn.Dropout(0.2)\n",
    "        self.out=nn.Linear(128,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=F.relu(self.fc3(x))\n",
    "        x=F.relu(self.fc4(x))\n",
    "        out=self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fcf657f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d4e9d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97572c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.to(dev);\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca6795f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=1027, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "  (fc3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc4): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "# Parameters= 4794113\n"
     ]
    }
   ],
   "source": [
    "model_details=str(net.to(torch.device(dev)))\n",
    "print(model_details)\n",
    "num_params=sum(p.numel() for p in net.parameters())\n",
    "print('# Parameters=', num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ba5b9bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files=['morgan/data10.npz','morgan/data11.npz','morgan/data12.npz','morgan/data13.npz','morgan/data14.npz','morgan/data15.npz',\n",
    "             'morgan/data16.npz','morgan/data17.npz','morgan/data18.npz','morgan/data19.npz']\n",
    "test_files=['morgan/data20.npz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d780fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files=get_files(['morgan/'])\n",
    "test_files=get_files(['morgan_validation/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7bb11e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "473"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3742e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "Epoch=1 Train_Loss=0.167275 Test_Loss=0.079275 Test AP=0.281868 Precision=0.241906 Recall=0.424797 F1=0.308266 ROC_AUC=0.914911\n",
      "....................\n",
      "Epoch=2 Train_Loss=0.159408 Test_Loss=0.073675 Test AP=0.299529 Precision=0.299274 Recall=0.397117 F1=0.341322 ROC_AUC=0.922195\n",
      "........"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Training loop\n",
    "for i in range(epoch_num):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_generator = generate_batches(train_files, batch_size=1024)\n",
    "    for batch in train_generator:\n",
    "        optimizer.zero_grad()\n",
    "        batch_morgan, batch_protein, batch_labels = batch\n",
    "        batch_morgan = np.concatenate((batch_morgan, batch_protein), 1)\n",
    "        score = net(torch.Tensor(batch_morgan).to(dev))\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(score, torch.Tensor(batch_labels)[:, None].to(dev), pos_weight=pos_weight)\n",
    "        train_loss += loss.cpu().item() * len(score)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        preds = []\n",
    "        labels = []\n",
    "        test_loss = 0\n",
    "        test_generator = generate_batches(test_files, batch_size=1024)\n",
    "        for batch in test_generator:\n",
    "            batch_morgan, batch_protein, batch_labels = batch\n",
    "            batch_morgan = np.concatenate((batch_morgan, batch_protein), 1)\n",
    "            score = net(torch.Tensor(batch_morgan).to(dev))\n",
    "            loss = torch.nn.functional.binary_cross_entropy_with_logits(score, torch.Tensor(batch_labels)[:, None].to(dev), pos_weight=pos_weight)\n",
    "            preds.append(torch.sigmoid(score).cpu().numpy())\n",
    "            labels.append(batch_labels)\n",
    "            test_loss += loss.cpu().item() * len(score)\n",
    "\n",
    "        preds = np.vstack(preds)[:, 0]\n",
    "        labels = np.hstack(labels)\n",
    "\n",
    "        # Calculate additional metrics\n",
    "        ap = average_precision_score(labels, preds)\n",
    "        precision = precision_score(labels, preds > 0.5)\n",
    "        recall = recall_score(labels, preds > 0.5)\n",
    "        f1 = f1_score(labels, preds > 0.5)\n",
    "        roc_auc = roc_auc_score(labels, preds)\n",
    "\n",
    "        print(f'\\nEpoch={i+1} Train_Loss={train_loss/len(labels):.6f} Test_Loss={test_loss/len(labels):.6f} '\n",
    "              f'Test AP={ap:.6f} Precision={precision:.6f} Recall={recall:.6f} F1={f1:.6f} ROC_AUC={roc_auc:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3947df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70300415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............\n",
      " Epoch=1 Train_Loss=1.003138149199302 Test_Loss=0.04882204769590944 Test AP=0.13260512288866608\n",
      "...............\n",
      " Epoch=2 Train_Loss=0.8343921044622662 Test_Loss=0.052541076241587104 Test AP=0.15806395912532023\n",
      "...............\n",
      " Epoch=3 Train_Loss=0.7964697278223664 Test_Loss=0.06023136017497704 Test AP=0.16564010288688769\n",
      "...............\n",
      " Epoch=4 Train_Loss=0.7719787498841771 Test_Loss=0.06954713337385077 Test AP=0.16598595814173905\n",
      "...............\n",
      " Epoch=5 Train_Loss=0.7594206354114563 Test_Loss=0.11244394741869336 Test AP=0.17529272770581467\n",
      "...............\n",
      " Epoch=6 Train_Loss=0.7488237145274264 Test_Loss=0.156620718493725 Test AP=0.1808955108795634\n",
      "...............\n",
      " Epoch=7 Train_Loss=0.7353306274759498 Test_Loss=0.32474393346613667 Test AP=0.17521899748532252\n",
      "...............\n",
      " Epoch=8 Train_Loss=0.7311725073708566 Test_Loss=0.4824021732787721 Test AP=0.18973572540806538\n",
      "...............\n",
      " Epoch=9 Train_Loss=0.7230835853912253 Test_Loss=0.5686870524970539 Test AP=0.19663832765747946\n",
      "...............\n",
      " Epoch=10 Train_Loss=0.7209700706965765 Test_Loss=1.3120358026004784 Test AP=0.19619271792446627\n"
     ]
    }
   ],
   "source": [
    "#this is the original trainning part, keep it for reference\n",
    "\n",
    "%%time\n",
    "pos_weight=torch.Tensor(np.array(5.0))\n",
    "epoch_num=10\n",
    "\n",
    "for i in range(epoch_num):\n",
    "    net.train()\n",
    "    train_loss=0\n",
    "    train_generator=generate_batches(train_files, batch_size=1024)\n",
    "    for batch in train_generator:\n",
    "        optimizer.zero_grad()\n",
    "        batch_morgan, batch_protein, batch_labels=batch\n",
    "        batch_morgan=np.concatenate((batch_morgan, batch_protein),1) #combine morgan fp with protein onehot encoding\n",
    "        score=net(torch.Tensor(batch_morgan).to(dev))\n",
    "        loss =  torch.nn.functional.binary_cross_entropy_with_logits(score, torch.Tensor(batch_labels)[:,None].to(dev), pos_weight=pos_weight)\n",
    "        train_loss+=loss.cpu().item()*len(score)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        preds=[]\n",
    "        labels=[]\n",
    "        test_loss=0\n",
    "        test_generator=generate_batches(test_files, batch_size=1024)\n",
    "        for batch in test_generator:\n",
    "            batch_morgan, batch_protein, batch_labels=batch\n",
    "            batch_morgan=np.concatenate((batch_morgan, batch_protein),1)\n",
    "            score=net(torch.Tensor(batch_morgan).to(dev))\n",
    "            loss =  torch.nn.functional.binary_cross_entropy_with_logits(score, torch.Tensor(batch_labels)[:,None].to(dev), pos_weight=pos_weight)\n",
    "            preds.append(torch.nn.functional.sigmoid(score))\n",
    "            labels.append(batch_labels)\n",
    "            test_loss+=loss.cpu().item()*len(score)\n",
    "            \n",
    "        preds=torch.vstack(preds).cpu().numpy()[:,0]\n",
    "        labels=np.hstack(labels)\n",
    "        ap=average_precision_score(labels, preds)\n",
    "        print('\\n Epoch={} Train_Loss={} Test_Loss={} Test AP={}'.format(i+1, train_loss/len(labels), test_loss/len(labels), ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bceed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2eeeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
